## 1.分布式系统

### 1.1 为什么要进行系统拆分

为什么要进行系统拆分？如何进行系统拆分？拆分后不用 Dubbo 可以吗？

### 1.2 分布式服务框架（RPC框架Dubbo）

- 说一下 Dubbo 的工作原理？注册中心挂了可以继续通信吗？
- Dubbo 支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？
- Dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？
- Dubbo 的 spi 思想是什么？
- 如何基于 Dubbo 进行服务治理、服务降级、失败重试以及超时重试？
- 分布式服务接口的幂等性如何设计（比如不能重复扣款）？
- 分布式服务接口请求的顺序性如何保证？
- 如何自己设计一个类似 Dubbo 的 RPC 框架？
- CAP 定理的 P 是什么？

### 1.3 分布式锁

- Zookeeper 都有哪些应用场景？
- 使用 Redis 如何设计分布式锁？使用 Zookeeper 来设计分布式锁可以吗？以上两种分布式锁的实现方式哪种效率比较高？

### 1.4 分布式事务

分布式事务了解吗？你们如何解决分布式事务问题的？TCC 如果出现网络连不通怎么办？XA 的一致性如何保证？

### 1.5 分布式会话

集群部署时的分布式 Session 如何实现？

### 1.6 流式处理框架（实时计算Flink）

## 2. 高并发架构

### 2.1 如何设计一个高并发系统

### 2.2 消息队列

#### 消息队列使用场景及优缺点

优点：消息队列的三个核心使用场景：解耦、异步、削峰

- 解耦：一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦，但是其实这个调用是不需要直接同步调用接口的，这里就可以考虑使用MQ进行系统解耦。通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，上游系统就跟其它下游系统彻底解耦了
- 异步：
- 削峰：典型的使用场景就是秒杀业务用于流量削峰场景

缺点：系统可用性降低，系统复杂度提高，一致性问题

#### 消息队列的推拉模式

在rocketmq里，consumer被分为2类：MQPullConsumer和MQPushConsumer，其实本质都是拉模式（pull），即consumer轮询从broker拉取消息。
区别是：
push方式里，consumer把轮询过程封装了，并注册MessageListener监听器，取到消息后，唤醒MessageListener的consumeMessage()来消费，对用户而言，感觉消息是被推送过来的。

pull方式里，取消息的过程需要用户自己写，首先通过打算消费的Topic拿到MessageQueue的集合，遍历MessageQueue集合，然后针对每个MessageQueue批量取消息，一次取完后，记录该队列下一次要取的开始offset，直到取完了，再换另一个MessageQueue。

#### 消息队列的高可用

##### RabbitMQ的高可用性

RabbitMQ**基于主从**（非分布式）实现高可用性。RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。

1. 单机模式：Demo级别

2. 普通集群模式（无高可用性）（只同步queue元数据，不同步消息数据）

   - 普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。
   - **创建的 queue，只会放在一个 RabbitMQ 实例上**，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。
   - 消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。
   
   缺点：
   
   - **MQ内部可能产生大量的数据传输。没做到所谓的分布式**，就是个普通集群。因为这种模式导致要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有**数据拉取的开销**，后者导致**单实例性能瓶颈**。
   - 可用性无保障，queue所在节点宕机，数据就丢了。如果开启了消息持久化，**让 RabbitMQ 落地存储消息的话**，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。

​		普通集群模式**主要是提高吞吐量的**，就是说让集群中多个节点来服务某个 queue 的读写操作。

3. 镜像集群模式（高可用性）（既同步queue元数据，也同步消息数据）

   - 在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会**存在于多个实例上**，就是说，每个 RabbitMQ 节点都有这个 queue 的一个**完整镜像**，包含 queue 的全部数据
   - 每次写消息到 queue 的时候，都会自动把**消息同步**到多个实例的 queue 上。

   缺点：
   
   - 性能开销太大，消息需要同步到所有机器上，导致网络带宽压力和消耗很重
   - 不是分布式的，**没有扩展性可言**，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并**没有办法线性扩展**

##### Kafka的高可用性

Kafka最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。一个 topic 的数据，是**分散放在多个机器上的，每个机器就放一部分数据**。

Partition分区数和Broker数关系：

1. 如果**Partition数等于Broker数**， Kafka集群将比较均衡
2. 如果**Partition数小于Broker数**，某个Broker节点上不存在当前topic的分区，Broker节点可能被闲置，最终导致Kafka集群吞吐率下降
3. 如果**Partition数大于Broker数**，抛异常：java.lang.IllegalArgumentException: Invalid partition given with record: 1 is not in the range [0...0]，建议将Partition数必须设置为Broker数的整数倍

HA机制：副本机制

1. Kafka 0.8 以后，提供了 HA 机制，就是 **replica（复制品） 副本机制**。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。
2. 所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。
3. 写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader原因如下：**要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题**，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。

副本机制是如何实现高可用的：

1. 如果某个 broker 宕机了，那个 broker上面的 partition 在其他机器上都有副本。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中**重新选举**一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。
2. **写数据**的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）
3. **消费**的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。

##### ==RocketMQ的高可用性==



#### 消息不被重复消费（消息消费的幂等性）

Kafka重复消费场景：

1. Kafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，**每隔一段时间**（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。
2. 但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。

需要结合业务来看，主要有如下思路：

- 比如拿个数据要写库，先根据主键查一下，如果这数据都有了，就别插入了，update 一下。
- 比如是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 比如不是上面两个场景，那做的稍微复杂一点，需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后消费者这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那就别处理了，保证别重复处理相同的消息即可。
- 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。

#### 消息的可靠性传输（如何处理消息丢失的问题）

##### RabbitMQ的可靠性处理

1. 生产者丢数据

   **开启RabbitMQ事务**（同步，消耗性能，吞吐量下降大，不推荐）

   - 生产者**发送数据之前**开启 RabbitMQ 事务 `channel.txSelect` ，然后发送消息；

   - 如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务 `channel.txRollback` ，然后重试发送消息；

   - 如果收到了消息，那么可以提交事务 `channel.txCommit` 。

   **开启confirm模式**（异步，推荐）

   - 在生产者端设置开启 `confirm` 模式之后，每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会回传一个 `ack` 消息，告知你这个消息 ok 了。

   - 如果 RabbitMQ 没能处理这个消息，会回调你的一个 `nack` 接口，告诉你这个消息接收失败，你可以重试。

   - 你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。

2. RabbitMQ丢数据

   **开启 RabbitMQ 的持久化**，消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，**恢复之后会自动读取之前存储的数据**，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，**可能导致少量数据丢失**，但是这个概率较小。

   设置持久化有**两个步骤**：

   - 创建 queue 的时候将其设置为持久化

   这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。

   - 消息设置为持久化

   发送消息的时候将消息的 `deliveryMode` 设置为 2，此时 RabbitMQ 就会将消息持久化到磁盘上去。

   **必须要同时设置这两个持久化才行**，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。

   极端情况：哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。

   所以，**持久化可以跟生产者那边的 `confirm` 机制配合起来**，只有**消息被持久化到磁盘之后，才会通知生产者 `ack`** 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 `ack` ，你也是可以自己重发的。

3. 消费者丢数据（**关闭RabbitMQ自动ack**）

   消费者如果丢失了数据，主要是因为你消费的时候，**刚消费到，还没处理，结果进程挂了**，比如重启了，RabbitMQ 认为你都消费了，这数据就丢了。
   
   这个时候得用 RabbitMQ 提供的 `ack` 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 `ack` ，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 `ack` 一把。这样的话，如果你还没处理完，就没有 `ack` 了。那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。

##### Kafka的可靠性处理

1. 消费者丢失数据

   消费者唯一丢数据的情况：消费者消费到消息后，消费者那边**自动提交了 offset**，Kafka 认为消费者已经消费好了这个消息，但其实消费者才刚准备处理这个消息，还没处理，自己就挂了，此时这条消息就丢失了。

   解决方法： Kafka 会自动提交 offset，那么只要**关闭自动提交** offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是**可能会有重复消费**，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。

2. Kafka丢失数据

   Kakfa丢失数据场景：Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。如果此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，还没有同步的数据就丢失了。

   解决方法：一般是要求起码设置如下 4 个参数：

   - 给 topic 设置 `replication.factor` 参数：这个值必须大于 1，要求**每个 partition 必须有至少 2 个副本**。
   - 在 Kafka 服务端设置 `min.insync.replicas` 参数：这个值必须大于 1，这个是要求**一个 leader 至少感知到有一个 follower 还跟自己保持联系**，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。
   - 在 producer 端设置 `acks=all` ：这个是要求每条数据，必须是**写入所有 replica 之后，才能认为是写成功了**。
   - 在 producer 端设置 `retries=MAX` （很大很大很大的一个值，无限次重试的意思）：这个是**要求一旦写入失败，就无限重试**，卡在这里了。

3. 生产者会不会丢失数据？

   如果按照上述的思路设置了 `acks=all` ，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。

##### ==RocketMQ的可靠性处理==



#### 消息的顺序性

消息顺序错乱的场景：

1. RabbitMQ

   一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。数据就出现了明显的错乱。

2. Kafka

   比如说我们建了一个 topic，有三个 partition。

   生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。

   消费者从 partition 中取出来数据的时候，也一定是有顺序的。

   到这里，顺序还是 ok 的，没有错乱。

   接着，我们在消费者里可能会搞**多个线程来并发处理消息**。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。

解决方案：

1. RabbitMQ

   - 拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；

   - 就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。

2. Kafka

   - 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。
   - 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。

#### 消息队列延时、过期失效、积压问题

如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？

本质针对的场景，都是说，可能你的消费端出了问题，不消费了；或者消费的速度极其慢。接着可能你的消息队列集群的磁盘都快写满了，都没人消费，这个时候怎么办？或者是这整个就积压了几个小时，你这个时候怎么办？或者是你积压的时间太长了，导致比如 RabbitMQ 设置了消息过期时间后就没了怎么办？

具体场景的解决办法：

1. 大量消息在MQ 里积压了几个小时了还没解决

   临时紧急扩容，具体操作步骤和思路如下：

   - 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。
   - 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。
   - 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，**消费之后不做耗时的处理**，直接均匀轮询写入临时建立好的 10 倍数量的 queue。
   - 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。
   - 等快速消费完积压数据之后，**得恢复原先部署的架构**，**重新**用原先的 consumer 机器来消费消息。

2.  MQ中的消息过期失效了

   消息队列设置过期时间之后，如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了，导致大量的数据直接丢失

   解决方法：

   **批量重导**。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。

3.  MQ都快写满了

   临时写程序，接入数据来消费，**消费一个丢弃一个，都不要了**，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据。

#### 如何设计一个消息队列

参照开源技术从如下几个角度进行考虑：

- 首先这个 mq 得**支持可伸缩性**，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下 kafka 的设计理念，broker -> topic -> partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？
- 其次你得考虑一下这个 mq 的**数据要不要落地磁盘**吧？那肯定要了，落磁盘才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。
- 其次你考虑一下你的 mq 的**可用性**啊？这个事儿，具体参考之前可用性那个环节讲解的 kafka 的高可用保障机制。多副本 -> leader & follower -> broker 挂了重新选举 leader 即可对外服务。
- 能不能支持**数据 0 丢失**啊？可以的，参考我们之前说的那个 kafka 数据零丢失方案。

### 2.3 搜索引擎（ElasticSearch）

### 2.4 缓存

#### 线程模型

Redis 内部使用文件事件处理器 `file event handler` ，这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。

文件事件处理器的结构包含 4 个部分：

- 多个 socket
- IO 多路复用程序（编译时自动选择系统中性能最高的I/O多路复用函数库）
- 文件事件分派器
- 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）

多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。

客户端与 Redis 的一次通信过程：

![image](https://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/FDA1084D00C34C1392074E780D4D76FD/9776)

1. 首先，Redis 服务端进程初始化的时候，会将 server socket 的 `AE_READABLE` 事件与连接应答处理器关联。

2. 客户端 socket01 向 Redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 `AE_READABLE` 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给**连接应答处理器**。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 `AE_READABLE` 事件与命令请求处理器关联。

3. 假设此时客户端发送了一个 `set key value` 请求，此时 Redis 中的 socket01 会产生 `AE_READABLE` 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 `AE_READABLE` 事件，由于前面 socket01 的 `AE_READABLE` 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 `key value` 并在自己内存中完成 `key value` 的设置。操作完成后，它会将 socket01 的 `AE_WRITABLE` 事件与命令回复处理器关联。

4. 如果此时客户端准备好接收返回结果了，那么 Redis 中的 socket01 会产生一个 `AE_WRITABLE` 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 `ok` ，之后解除 socket01 的 `AE_WRITABLE` 事件与命令回复处理器的关联。

为什么 Redis 单线程模型也能效率这么高？

- 纯内存操作。
- 核心是基于非阻塞的 IO 多路复用机制。
- C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。
- 单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。

**注意！** Redis 6.0 之后的版本抛弃了单线程模型这一设计，**原本使用单线程运行的 Redis 也开始选择性地使用多线程模型**。

前面还在强调 Redis 单线程模型的高效性，现在为什么又要引入多线程？这其实说明 Redis 在有些方面，单线程已经不具有优势了。因为读写网络的 Read/Write 系统调用在 Redis 执行期间占用了大部分 CPU 时间，如果把网络读写做成多线程的方式对性能会有很大提升。

**Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程。** 之所以这么设计是不想 Redis 因为多线程而变得复杂，需要去控制 key、lua、事务、LPUSH/LPOP 等等的并发问题。

#### 五个对象及底层数据结构

底层数据结构

| 对象所使用的底层数据结构 | OBJECT ENCODING 命令输出 |
| :--- | :--- |
| 整数 | int |
| embstr编码的简单动态字符串（SDS） | embstr |
| 简单动态字符串 | raw |
| 字典 | hashtable |
| 双端链表 | linkedlist |
| 压缩列表 | ziplist |
| 整数集合 | intset |
| 跳跃表和字典 | skiplist |

简单动态字符串（SDS）

- 常数时间复杂度获取字符串长度
- 杜绝缓冲区溢出：先扩展再拼接
- 减少修改时的内存分配次数
- 二进制安全
- 兼容部分C字符串函数

五种对象的底层实现

| 类型                  | 编码                | 备注                                                         |
| --------------------- | ------------------- | ------------------------------------------------------------ |
| 字符串（String）      | int，raw，embstr    | 普通的 set 和 get，做简单的 KV 缓存                          |
| 有序列表（List）      | ziplist，linkedlist | 通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类<br />通过 lrange 命令，读取某个闭区间内的元素<br />实现简单的消息队列 |
| 哈希对象（Hash）      | ziplist，hashtable  | 可存储结构化的数据，比如一个对象（无对象嵌套）               |
| 集合对象（ Set）      | intset，hashtable   | 无序集合，自动去重<br />基于 set 可以实现交集、并集、差集的操作 |
| 有序集合对象（ ZSet） | ziplist，skiplist   | 排序的 set，去重但可以排序                                   |


#### 过期删除机制，淘汰机制

Redis 过期策略是：**定期删除+惰性删除**。

**定期删除**：Redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。

**惰性删除**：定期删除可能会导致很多过期 key 到了时间并没有被删除掉，这种情况下走惰性删除。即获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。

**内存淘汰机制**：如果定期删除漏掉了很多过期 key，然后也没及时去查，也就没走惰性删除，如果大量过期 key 堆积在内存里，有可能会导致 Redis 内存块耗尽，此时走内存淘汰机制

Redis 内存淘汰机制有以下几个：

- noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。
- **allkeys-lru**：当内存不足以容纳新写入数据时，在**键空间**中，移除最近最少使用的 key（这个是**最常用**的）。
- allkeys-random：当内存不足以容纳新写入数据时，在**键空间**中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。
- volatile-lru：当内存不足以容纳新写入数据时，在**设置了过期时间的键空间**中，移除最近最少使用的 key（这个一般不太合适）。
- volatile-random：当内存不足以容纳新写入数据时，在**设置了过期时间的键空间**中，**随机移除**某个 key。
- volatile-ttl：当内存不足以容纳新写入数据时，在**设置了过期时间的键空间**中，有**更早过期时间**的 key 优先移除。

利用已有的 JDK 数据结构实现一个 Java 版的 LRU：

```Java
class LRUCache<K, V> extends LinkedHashMap<K, V> {
    private final int CACHE_SIZE;

    /**
     * 传递进来最多能缓存多少数据
     *
     * @param cacheSize 缓存大小
     */
    public LRUCache(int cacheSize) {
        // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最早访问的放在尾部。
        super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true);
        CACHE_SIZE = cacheSize;
    }

     /**
     * LinkedHashMap自带的判断是否删除最老的元素方法，默认返回false，即不删除老数据
     * 我们要做的就是重写这个方法，当满足一定条件时删除老数据
     * 钩子方法，通过put新增键值对的时候，若该方法返回true
     * 便移除该map中最老的键和值
     */
    @Override
    protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
        // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。
        return size() > CACHE_SIZE;
    }
}
```

#### 持久化

1. Redis 持久化的两种方式

   - RDB：RDB 持久化机制，是对 Redis 中的数据执行**周期性**的持久化，
   - AOF：AOF 机制对每条写入命令作为日志，以 `append-only` 的模式写入一个日志文件中，在 Redis 重启的时候，可以通过**回放** AOF 日志中的写入指令来重新构建整个数据集。

   如果同时使用 RDB 和 AOF 两种持久化机制，那么在 Redis 重启的时候，会优先使用 **AOF** 来重新构建数据，因为 AOF 文件更新频率更高，**数据更加完整**。

   只有在AOF持久化功能处于关闭状态时，服务器才会使用RDB文件来还原数据库状态。

2. RDB 

   RDB实现

   - 通过SAVE和BGSAVE命令生成RDB文件
   - SAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕，服务器进程阻塞期间，不能处理任何命令请求。
   - BGSAVE命令会派生出一个子进程，由子进程执行磁盘 IO 操作负责创建RDB文件，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。服务器进程（父进程）继续处理命令请求。可以通过设置save选项（设置一段时间内的修改次数），让服务器每隔一段时间自动执行一次BGSAVE命令
   - RDB文件的载入是在服务器启动时自动执行，没有专门的载入命令，只要启动时检测到RDB文件存在，就会自动载入RDB文件。在载入RDB文件期间，服务器会一直处于阻塞状态直到载入工作完成

   RDB优缺点

   - RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 Redis 的数据，这种多个数据文件的方式，**非常适合做冷备**，以预定好的备份策略来定期备份 Redis 中的数据。
   - RDB 对 Redis 对外提供的读写服务，影响非常小，可以让 Redis **保持高性能**，因为 Redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。
   - 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 Redis 进程，更加快速。
   - 如果想要在 Redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 Redis 进程宕机，那么会丢失最近 5 分钟的数据。
   - RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。

3. AOF 

   AOF 实现

   - AOF持久化功能的实现分为三个步骤：命令追加（append），文件写入，文件同步（sync）

   - 服务器在执行完一个写命令后，以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾，之后再定期写入并同步到AOF文件
   - appendfsync选项的不同值对AOF持久化功能的安全性以及Redis服务器的性能有很大的影响
   - AOF重写可以产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一致，但体积更小。
   - 在执行BGREWRITEAOF命令时，Redis服务器会维护一个AOF重写缓冲区，该缓冲区会在子进程创建新AOF文件期间，记录服务器执行的所有命令。当Redis服务器执行完一个写命令后，它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区的所有内容追加到新AOF文件的末尾，使得新旧两个AOF文件所保存的数据库状态一致。最后，服务器用新的AOF文件替换旧的AOF文件，以此来完成AOF文件重写操作

   AOF 优缺点

   - AOF 可以**更好的保护数据不丢失**，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次 `fsync` 操作，最多丢失 1 秒钟的数据。
   - AOF 日志文件以 `append-only` 模式写入，所以没有任何磁盘寻址的开销，**写入性能非常高**，而且文件不容易破损，即使文件尾部破损，也很容易修复。
   - AOF 日志文件即使过大的时候，出现后台重写操作，也**不会影响客户端的读写**。
   - AOF 日志文件的命令通过可读较强的方式进行记录，这个特性非常**适合做灾难性的误删除的紧急恢复**。比如某人不小心用 `flushall` 命令清空了所有数据，只要这个时候后台 `rewrite` 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 `flushall` 命令给删了，然后再将该 `AOF` 文件放回去，就可以通过恢复机制，自动恢复所有数据。
   - 对于同一份数据来说，**AOF 日志文件通常比 RDB 数据快照文件更大**。
   - AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 `fsync` 一次日志文件，当然，每秒一次 `fsync` ，性能也还是很高的。（如果实时写入，那么 QPS 会大降，Redis 性能会大大降低）
   - 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是**基于当时内存中的数据进行指令的重新构建**，这样健壮性会好很多。

4. RDB和AOF选择

   - 不要仅仅使用 RDB，因为那样会导致丢失很多数据；
   - 也不要仅仅使用 AOF，因为那样有两个问题：第一，通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug；
   - Redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。

#### 主从复制

主从架构可实现 Redis 高并发，一主多从，读写分离

Redis replication -> 主从架构 -> 读写分离 -> 水平扩容支撑读高并发

##### Redis同步功能实现原理

复制功能分为同步（sync）和命令传播（command propagate）两个操作。

1. 旧版主从复制（Redis 2.8版本以前）

   旧版复制缺陷：从服务器对主服务器的同步操作是通过向SYNC命令来完成，这个命令需要主服务器执行BGSAVE命令来生成RDB文件，然后将生成的RDB文件发送给从服务器，这个发送操作会耗费从服务器大量的网络资源，并对主服务器响应命令请求的时间产生影响。在载入接收到的RDB文件期间，从服务器是阻塞的。断线后重连是通过发送SYNC命令实现的，效率低下。

2. 新版主从复制（Redis 2.8版本以后）

   使用PSYNC命令代替了SYNC命令来执行复制时的同步操作。PSYNC命令具有完整重同步（`full resynchronization`）和部分重同步（`partial resynchronization`）两种模式

   完整重同步：

   - master 执行 bgsave ，在本地生成一份 rdb 快照文件。
   - master node 将 rdb 快照文件发送给 slave node，如果 rdb 复制时间超过 60秒（repl-timeout），那么 slave node 就会认为复制失败，可以适当调大这个参数(对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s)
   - master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。
   - 如果在复制期间，内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败。

   ```
   client-output-buffer-limit slave 256MB 64MB 60
   ```

   - slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时**基于旧的数据版本**对外提供服务。
   - 如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。

   部分重同步：

   - 如果全量复制过程中，master-slave 网络连接断掉，那么 slave 重新连接 master 时，会触发增量复制。
   - master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，默认 backlog 就是 1MB。
   - master 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的。

采用了主从架构，那么建议必须**开启** master node 的[持久化](https://gitee.com/Doocs/advanced-java/blob/master/docs/high-concurrency/redis-persistence.md)，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。

##### 主从复制的核心原理

1. 当启动一个 slave node 的时候，它会发送一个 `PSYNC` 命令给 master node。

2. 如果这是 slave node 初次连接到 master node，那么会触发一次 `full resynchronization` 全量复制。
3. 此时 master 会启动一个后台线程，开始生成一份 `RDB` 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。
4.  `RDB` 文件生成完毕后， master 会将这个 `RDB` 发送给 slave，slave 会先**写入本地磁盘，然后再从本地磁盘加载到内存**中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。
5. slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。（部分重同步操作）

![image](https://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/305EB887D1E5455796B9132F2D21187D/9780)

主从复制的断点续传（通过部分重同步实现）：

1. 部分重同步功能由三个部分构成：
   - 主服务器的复制偏移量（replication offset）和从服务器的复制偏移量
   - 主服务器的复制积压缓冲区（repliaction backlog）（由主服务器维护的默认1MB的先进先出队列）
   - 服务器的运行 ID（run ID）

2. master node 会在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 `resynchronization` 。

无磁盘化复制：

- master 在内存中直接创建 `RDB` ，然后发送给 slave，不会在自己本地落地磁盘了。只需要在配置文件中开启 `repl-diskless-sync yes` 即可。

过期key处理：

- slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。

##### 主从复制的流程

通过向从服务器发送SLAVEOF命令，可以让一个从服务器去复制一个主服务器。

1. 设置主服务器的地址和端口：slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的 `host` 和 `ip` ，保存完成后，从服务器返回OK，实际的复制工作在返回OK后开始执行

2. 建立套接字连接：slave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接

3. 发送PING命令：连接建立后，slave node 发送 `ping` 命令给 master node。如果从服务器读取到PONG回复，表示网络连接正常

4. 身份验证：接收到PONG回复后，

   - 如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。
   - 如果没有设置 requirepass，则不需要身份验证。

   在需要进行身份验证的情况下，从服务器将发送一条AUTH命令给主服务器，命令参数为从服务器requirepass选项的值

5. 发送端口信息：在身份验证后，从服务器将执行REPLCONF命令，向主服务器发送从服务器的监听端口号

6. 同步：在这一步，从服务器向主服务器发送PSYNC命令，master node **第一次执行全量复制**，将所有数据发给 slave node

7. 命令传播：在后续，master node 每次接收到写命令后，先在内部写入数据，再将写命令异步发送给 slave node。

<img src="https://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/WEBRESOURCE402eefd8f9cc1db14d97b4d1e10b2b16/9786" alt="image" style="zoom: 67%;" />

心跳检查：

主从节点互相都会发送 heartbeat 信息。master 默认每隔 10秒 发送一次 heartbeat，slave node 每隔 1秒 发送一个 heartbeat。

命令为REPLCONF ACK。主要有三个作用：

1. 检测主从服务器的连接状态
2. 辅助实现min-salves选项
3. 检测命令丢失

#### 哨兵（Sentinel）

哨兵是运行在特殊模式下的Redis服务器，使用了和普通模式不同的命令表

##### 哨兵原理

1. 启动并初始化Sentinel

   Sentinel会读入用户指定的配置文件，为每个要被监视的主服务器创建相应的实例结构，并创建连向主服务器的命令连接和订阅连接，其中命令连接用于向主服务器发送命令请求，订阅连接用于接收指定频道的消息

2. 获取主服务器和从服务器信息

   Sentinel通过向主服务器发送INFO命令来获得主服务器属下所有从服务器的地址信息，并为这些从服务器创建相应的实例结构，以及连向这些从服务器的命令连接和订阅连接。

   一般情况下，Sentinel以每十秒一次的频率向被监视的主服务器和从服务器发送INFO命令，当主服务器处于下线状态，或者Sentinel正在对主服务器进行故障转移操作时，Sentinel向从服务器发送INFO命令的频率会改为每秒一次。

3. 向主从服务器发送信息

   对于监视同一个主服务器和从服务器的多个Sentinel来说，它们会以每两秒一次的频率，向被监视服务器的`__sentinel__:hello`频道发送消息来向其他Sentinel宣告自己的存在，消息内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。

   每个哨兵也会去**监听**自己监控的每个 master+slaves 对应的 `__sentinel__:hello` channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。

   每个哨兵还会跟其他哨兵交换对 `master` 的监控配置，互相进行监控配置的同步。

4. 接收来自主从服务器的频道消息

   哨兵互相之间的发现，是通过 Redis 的 `pub/sub` 系统实现的，每个Sentinel也会从`__sentinel__:hello`频道接收其他Sentinel发来的信息，并根据这些信息为其他Sentinel创建相应的实例结构以及命令连接

   Sentinel只会与主服务器和从服务器创建命令连接和订阅连接，Sentinel与Sentinel之间只创建命令连接

5. 检测主观下线（sdown）状态

   Sentinel以每秒一次的频率向实例（包括主服务器、从服务器、其他Sentinel）发送PING命令，并根据实例对PING命令的回复来判断实例是否在线，当一个实例在指定的时长中连续向Sentinel发送无效回复时，Sentinel会将这个实例判断为主观下线

6. 检测客观下线（odown）状态

   当Sentinel将一个主服务器判断为主观下线时，它会向同样监视这个主服务器的其他Sentinel进行询问，看它们是否同意这个主服务器已经进入主观下线状态。

   当Sentinel收集到足够多（大于等于设置的quorum 数量）的主观下线投票之后，它会将主服务器判断为客观下线，并发起一次针对主服务器的故障转移操作

7. 选举领头Sentinel

   <img src="https://note.youdao.com/yws/public/resource/aba0f08fcb448be8bda00fbd1ddd049d/xmlnote/WEBRESOURCEd10d985e35ad93752d500720bd73a01c/9789" alt="image" style="zoom:50%;" />

8. 故障转移

   在选举出领头Sentinel之后，领头Sentinel将对已下线的主服务器执行故障转移操作，包含以下三个步骤：

   1）在已下线的主服务器属下的从服务器里面，挑选出一个从服务器，并将其转换为主服务器

   2）让已下线主服务器属下的所有从服务器改为复制新的主服务器

   3）将已下线主服务器设置为新的主服务器的从服务器，当这个旧的主服务器重新上线时，它就会成为新的主服务器的从服务器

##### 哨兵的核心知识

- 哨兵至少需要 3 个（>2，不能等于）实例，来保证自己的健壮性。

- 哨兵 + Redis 主从的部署架构，**不能保证数据零丢失**，只能保证 Redis 集群的高可用性。

- 对于哨兵 + Redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。

  哨兵集群必须部署 2 个以上节点，如果哨兵集群仅仅部署了 2 个哨兵实例，quorum = 1。

  ```
  +----+         +----+
  | M1 |---------| R1 |
  | S1 |         | S2 |
  +----+         +----+
  ```

  配置 `quorum=1` ，如果 master 宕机， s1 和 s2 中只要有 1 个哨兵认为 master 宕机了，就可以进行切换，同时 s1 和 s2 会选举出一个哨兵来执行故障转移。但是同时这个时候，需要 majority，也就是大多数哨兵都是运行的。

  ```
  2 个哨兵，majority=2
  3 个哨兵，majority=2
  4 个哨兵，majority=2
  5 个哨兵，majority=3
  ...
  ```

  如果此时仅仅是 M1 进程宕机了，哨兵 s1 正常运行，那么故障转移是 OK 的。但是如果是整个 M1 和 S1 运行的机器宕机了，那么哨兵只有 1 个，此时就没有 majority 来允许执行故障转移，虽然另外一台机器上还有一个 R1，但是故障转移不会执行。

  经典的 3 节点哨兵集群是这样的：

  ```
         +----+
         | M1 |
         | S1 |
         +----+
            |
  +----+    |    +----+
  | R2 |----+----| R3 |
  | S2 |         | S3 |
  +----+         +----+
  ```

  配置 `quorum=2` ，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移。

##### 哨兵主备切换的数据丢失问题

1. 数据丢失的两种情况

   - 异步复制导致的数据丢失

   因为 master->slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。

   - 脑裂导致的数据丢失

   脑裂，也就是说，某个 master 所在机器突然**脱离了正常的网络**，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会**认为** master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的**脑裂**。

   此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。

2. 数据丢失问题的解决方案

   进行如下配置：

   ```
   min-slaves-to-write 1
   min-slaves-max-lag 10
   ```

   表示，要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒。如果说一旦所有的 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了。

   - 减少异步复制数据的丢失

   有了 `min-slaves-max-lag` 这个配置，就可以确保说，一旦 slave 复制数据和 ack 延时太长，就认为可能 master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 master 宕机时由于部分数据未同步到 slave 导致的数据丢失降低的可控范围内。

   - 减少脑裂的数据丢失

   如果一个 master 出现了脑裂，跟其他 slave 丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的 slave 发送数据，而且 slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。

##### 选举算法和故障转移

1. slave->master 选举算法

   如果一个 master 被认为 odown 了，而且 majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来，会考虑 slave 的一些信息：

   - 跟 master 断开连接的时长
   - slave 优先级
   - 复制 offset
   - run id

   如果一个 slave 跟 master 断开连接的时间已经超过了 `down-after-milliseconds` 的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master。

   ```
   (down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state
   ```

   接下来会对 slave 进行排序：

   - 按照 slave 优先级进行排序，slave priority 越低，优先级就越高。
   - 如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高。
   - 如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave。

2. quorum 和 majority

   每次一个哨兵要做主备切换，首先需要 quorum 数量的哨兵认为 odown，然后选举出一个哨兵来做切换，这个哨兵还需要得到 majority 哨兵的授权，才能正式执行切换。

   如果 quorum < majority，比如 5 个哨兵，majority 就是 3，quorum 设置为 2，那么就 3 个哨兵授权就可以执行切换。

   但是如果 quorum >= majority，那么必须 quorum 数量的哨兵都授权，比如 5 个哨兵，quorum 是 5，那么必须 5 个哨兵都同意授权，才能执行切换。

3. configuration epoch

   哨兵会对一套 Redis master+slaves 进行监控，有相应的监控的配置。

   执行切换的那个哨兵，会从要切换到的新 master（salve->master）那里得到一个 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的。

   如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号。

4. configuration 传播

   哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 `pub/sub` 消息机制。

   这里之前的 version 号就很重要了，因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的。其他的哨兵都是根据版本号的大小来更新自己的 master 配置的。

#### Redis集群、一致性哈希

想要在实现高并发的同时，容纳大量的数据，那么就需要 Redis 集群

16384个槽

Redis 集群模式的工作原理能说一下么？

在集群模式下，Redis 的 key 是如何寻址的？

分布式寻址都有哪些算法？

了解一致性 hash 算法吗？

如何动态增加和删除一个节点？

#### 事务

Redis 的并发竞争问题是什么？如何解决这个问题？

了解 Redis 事务的 CAS 方案吗？

#### 缓存并发竞争

解释：多个客户端写一个 key，如果顺序错了，数据就不对了。但是顺序我们无法控制。

解决方案：使用分布式锁，例如 zk，同时加入数据的时间戳。同一时刻，只有抢到锁的客户端才能写入，同时，写入时，比较当前数据的时间戳和缓存中数据的时间戳。

#### 缓存雪崩、穿透、击穿

1. 缓存雪崩
   - 事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。
   - 事中：本地 ehcache 缓存 + hystrix 限流&降级，避免 MySQL 被打死
   - 事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。

2. 缓存穿透

   每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 `set -999 UNKNOWN` 。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。

3. 缓存击穿
   - 若缓存的数据是基本不会发生更新的，则可尝试将该热点数据设置为永不过期。
   - 若缓存的数据更新不频繁，且缓存刷新的整个流程耗时较少的情况下，则可以采用基于 Redis、zookeeper 等分布式中间件的分布式互斥锁，或者本地互斥锁以保证仅少量的请求能请求数据库并重新构建缓存，其余线程则在锁释放后能访问到新缓存。
   - 若缓存的数据更新频繁或者在缓存刷新的流程耗时较长的情况下，可以利用定时线程在缓存过期前**主动地重新构建缓存或者延后缓存的过期时间**，以保证所有的请求能一直访问到对应的缓存。

#### 数据库和缓存双写一致性

- 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。
- 更新的时候，**先更新数据库，然后再删除缓存**。

### 2.5 分库分表

- 为什么要分库分表（设计高并发系统的时候，数据库层面该如何设计）？用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？你们具体是如何对数据库如何进行垂直拆分或水平拆分的？
- 现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？
- 如何设计可以动态扩容缩容的分库分表方案？
- 分库分表之后，id 主键如何处理？

### 2.6 读写分离

- 如何实现 MySQL 的读写分离？MySQL 主从复制原理是啥？如何解决 MySQL 主从同步的延时问题？

## 3. 高可用架构

### 3.1 如何设计一个高可用系统

### 3.2 限流

### 3.3 熔断

### 3.4 降级

